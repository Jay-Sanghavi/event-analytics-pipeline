# Capstone Deployment Checklist\n\n## Pre-Deployment Verification\n\n### Local Verification\n- [ ] All files present:\n  - [ ] capstone-starter.cfn.yaml\n  - [ ] build_pipeline.ipynb\n  - [ ] queries.sql\n  - [ ] generate_sample_data.py\n  - [ ] sample-data/ (with .jsonl.gz files)\n  - [ ] README.md\n  - [ ] PIPELINE_DOCUMENTATION.md\n  - [ ] COMPLETION_SUMMARY.md\n  - [ ] QUERIES_REFERENCE.md\n\n- [ ] AWS credentials configured\n  ```bash\n  aws sts get-caller-identity\n  ```\n\n- [ ] Region set to us-west-2\n  ```bash\n  aws configure get region\n  ```\n\n---\n\n## Phase 1: CloudFormation Deployment (30 minutes)\n\n### Step 1.1: Validate Template\n```bash\naws cloudformation validate-template \\\n  --template-body file://capstone-starter.cfn.yaml \\\n  --region us-west-2\n```\n- [ ] Output: `TemplateDescription` and parameter list\n- [ ] No validation errors\n\n### Step 1.2: Create Stack\n```bash\naws cloudformation create-stack \\\n  --stack-name capstone-jsanghvi \\\n  --template-body file://capstone-starter.cfn.yaml \\\n  --parameters ParameterKey=StudentId,ParameterValue=jsanghvi \\\n  --region us-west-2\n```\n- [ ] Output: Stack ID\n- [ ] Save Stack ID\n\n### Step 1.3: Wait for Stack Creation\n```bash\naws cloudformation wait stack-create-complete \\\n  --stack-name capstone-jsanghvi \\\n  --region us-west-2\n```\n- [ ] Command completes (may take 5-10 minutes)\n- [ ] No errors in output\n\n### Step 1.4: Verify Stack Resources\n```bash\naws cloudformation describe-stack-resources \\\n  --stack-name capstone-jsanghvi \\\n  --region us-west-2\n```\n- [ ] SourceEventsBucket created\n- [ ] EventGeneratorFunction created\n- [ ] EventGeneratorSchedule created\n- [ ] AnalyticsOutputBucket created\n- [ ] Databases created (Bronze, Silver, Gold)\n- [ ] Tables created\n\n### Step 1.5: Get Stack Outputs\n```bash\naws cloudformation describe-stacks \\\n  --stack-name capstone-jsanghvi \\\n  --region us-west-2 \\\n  --query 'Stacks[0].Outputs'\n```\n- [ ] Note SOURCE_BUCKET name\n- [ ] Note ANALYTICS_BUCKET name\n- [ ] Note database names\n\n---\n\n## Phase 2: Event Generation (5+ minutes)\n\n### Step 2.1: Wait for First Event Generation\n- [ ] Lambda runs on 5-minute schedule\n- [ ] Wait at least 10 minutes from stack creation\n- [ ] Or manually invoke Lambda:\n  ```bash\n  aws lambda invoke \\\n    --function-name capstone-event-generator-jsanghvi \\\n    --region us-west-2 \\\n    /tmp/response.json\n  cat /tmp/response.json\n  ```\n\n### Step 2.2: Verify Events in Source Bucket\n```bash\naws s3 ls s3://capstone-events-jsanghvi-{ACCOUNT}/events/ \\\n  --recursive --region us-west-2\n```\n- [ ] Files exist with Hive partitioning structure\n- [ ] Files have .jsonl.gz extension\n- [ ] Multiple files if multiple cycles completed\n\n### Step 2.3: Sample Event Data\n```bash\naws s3 cp 's3://capstone-events-jsanghvi-{ACCOUNT}/events/year=2025/month=12/day=08/hour=18/minute=30/events-20251208-183000.jsonl.gz' - | \\\n  gunzip | head -5\n```\n- [ ] Valid JSON objects returned\n- [ ] Contains expected fields (timestamp, user_id, event_type, etc.)\n- [ ] No errors\n\n---\n\n## Phase 3: Glue Infrastructure Setup (10 minutes)\n\n### Step 3.1: Prepare build_pipeline.ipynb\n- [ ] Update STUDENT_ID variable if needed (should be 'jsanghvi')\n- [ ] Verify REGION is 'us-west-2'\n- [ ] Save any changes\n\n### Step 3.2: Run Notebook\n```bash\njupyter notebook build_pipeline.ipynb\n```\n- [ ] Browser opens with notebook\n- [ ] Execute cells in order\n\n### Step 3.3: Cell 1 - AWS Setup\n- [ ] Credentials verified\n- [ ] Account ID printed\n- [ ] Stack name printed\n\n### Step 3.4: Cell 2 - Get Stack Outputs\n- [ ] All resources found\n- [ ] Bucket names printed\n- [ ] Database names printed\n\n### Step 3.5: Cell 3-4 - Create Job Script\n- [ ] Script generated\n- [ ] Script uploaded to S3\n- [ ] S3 path printed\n\n### Step 3.6: Cell 5 - Create Glue Job\n- [ ] Job created or updated\n- [ ] Success message printed\n- [ ] Job name: capstone-etl-jsanghvi\n\n### Step 3.7: Cell 6 - Verify Setup\n- [ ] Summary printed\n- [ ] All buckets and databases listed\n\n---\n\n## Phase 4: Execute Glue ETL Job (5-15 minutes)\n\n### Step 4.1: Start Job Run\n```bash\naws glue start-job-run \\\n  --job-name capstone-etl-jsanghvi \\\n  --region us-west-2\n```\n- [ ] Output: Run ID\n- [ ] Save Run ID\n\n### Step 4.2: Monitor Job Progress\n```bash\nwatch -n 10 'aws glue get-job-run \\\n  --job-name capstone-etl-jsanghvi \\\n  --run-id {RUN_ID} \\\n  --region us-west-2 \\\n  --query \"JobRun.{State:JobRunState,Duration:ExecutionTime}\"'\n```\n- [ ] Initial state: RUNNING\n- [ ] After 5-15 mins: SUCCEEDED\n- [ ] No FAILED state\n\n### Step 4.3: Check CloudWatch Logs\n```bash\naws logs tail /aws-glue/jobs/capstone-etl-jsanghvi --follow\n```\n- [ ] Job initialization messages\n- [ ] \"Loading events\" message\n- [ ] \"Bronze layer written\" message\n- [ ] \"Silver layer written\" message\n- [ ] \"Gold layer written\" message (5 times)\n- [ ] \"ETL Job Complete\" message\n- [ ] No ERROR or EXCEPTION messages\n\n### Step 4.4: Verify Outputs in S3\n```bash\naws s3 ls s3://capstone-analytics-jsanghvi-{ACCOUNT}/ --recursive\n```\n- [ ] bronze/events/ folder exists with Parquet files\n- [ ] silver/events_cleaned/ folder exists with Parquet files\n- [ ] gold/ folder with 5 subfolders:\n  - [ ] conversion_funnel/\n  - [ ] hourly_revenue/\n  - [ ] top_products/\n  - [ ] category_performance/\n  - [ ] user_activity/\n\n---\n\n## Phase 5: Query Validation (10 minutes)\n\n### Step 5.1: Create Athena Output Bucket\n```bash\naws s3 mb s3://capstone-analytics-jsanghvi-{ACCOUNT}/athena-results/ \\\n  --region us-west-2\n```\n- [ ] Bucket created (or already exists)\n\n### Step 5.2: Query 1 - Conversion Funnel\n```bash\naws athena start-query-execution \\\n  --query-string \"SELECT * FROM gold_jsanghvi.conversion_funnel LIMIT 5\" \\\n  --query-execution-context Database=gold_jsanghvi \\\n  --result-configuration OutputLocation=s3://capstone-analytics-jsanghvi-{ACCOUNT}/athena-results/ \\\n  --region us-west-2\n```\n- [ ] Query ID returned\n- [ ] Query succeeds (check status)\n- [ ] Results contain: product_id, views, purchases, conversion rates\n- [ ] All conversion rates are between 0 and 1\n\n### Step 5.3: Query 2 - Hourly Revenue\n```bash\naws athena start-query-execution \\\n  --query-string \"SELECT * FROM gold_jsanghvi.hourly_revenue LIMIT 5\" \\\n  --query-execution-context Database=gold_jsanghvi \\\n  --result-configuration OutputLocation=s3://capstone-analytics-jsanghvi-{ACCOUNT}/athena-results/ \\\n  --region us-west-2\n```\n- [ ] Query succeeds\n- [ ] Results contain: event_date, event_hour, total_revenue, transaction_count\n- [ ] Revenue values are positive\n- [ ] Hours are 0-23\n\n### Step 5.4: Query 3 - Top Products\n```bash\naws athena start-query-execution \\\n  --query-string \"SELECT * FROM gold_jsanghvi.top_products\" \\\n  --query-execution-context Database=gold_jsanghvi \\\n  --result-configuration OutputLocation=s3://capstone-analytics-jsanghvi-{ACCOUNT}/athena-results/ \\\n  --region us-west-2\n```\n- [ ] Query succeeds\n- [ ] Results contain exactly 10 rows\n- [ ] Rank goes 1-10\n- [ ] View counts are in descending order\n\n### Step 5.5: Query 4 - Category Performance\n```bash\naws athena start-query-execution \\\n  --query-string \"SELECT * FROM gold_jsanghvi.category_performance LIMIT 10\" \\\n  --query-execution-context Database=gold_jsanghvi \\\n  --result-configuration OutputLocation=s3://capstone-analytics-jsanghvi-{ACCOUNT}/athena-results/ \\\n  --region us-west-2\n```\n- [ ] Query succeeds\n- [ ] Results contain: event_date, category, event_count\n- [ ] Multiple rows per date (different categories)\n- [ ] Event counts are positive\n\n### Step 5.6: Query 5 - User Activity\n```bash\naws athena start-query-execution \\\n  --query-string \"SELECT * FROM gold_jsanghvi.user_activity LIMIT 10\" \\\n  --query-execution-context Database=gold_jsanghvi \\\n  --result-configuration OutputLocation=s3://capstone-analytics-jsanghvi-{ACCOUNT}/athena-results/ \\\n  --region us-west-2\n```\n- [ ] Query succeeds\n- [ ] Results contain: event_date, unique_users, unique_sessions\n- [ ] Unique sessions >= unique users (at least 1 session per user)\n- [ ] Positive user counts\n\n### Step 5.7: Run All Queries from queries.sql\n```bash\naws athena start-query-execution \\\n  --query-string file://queries.sql \\\n  --query-execution-context Database=gold_jsanghvi \\\n  --result-configuration OutputLocation=s3://capstone-analytics-jsanghvi-{ACCOUNT}/athena-results/ \\\n  --region us-west-2\n```\n- [ ] All 5 queries execute without errors\n- [ ] Results make business sense\n\n---\n\n## Phase 6: Documentation & Submission\n\n### Step 6.1: Verify All Files\n- [ ] capstone-starter.cfn.yaml - Extended with pipeline resources\n- [ ] build_pipeline.ipynb - Creates Glue infrastructure\n- [ ] queries.sql - 5 SQL queries\n- [ ] All supporting documentation files\n\n### Step 6.2: Document Key Values\n- [ ] Account ID: _______________\n- [ ] Source Bucket: _______________\n- [ ] Analytics Bucket: _______________\n- [ ] Stack Name: capstone-jsanghvi\n- [ ] Glue Job Name: capstone-etl-jsanghvi\n- [ ] First successful Glue run date: _______________\n\n### Step 6.3: Prepare for Submission\n- [ ] CloudFormation template ready\n- [ ] Notebook ready (with account-specific values)\n- [ ] queries.sql ready\n- [ ] Analytics bucket URI ready: `s3://capstone-analytics-jsanghvi-{ACCOUNT}`\n- [ ] Google Doc blog post created (in progress)\n\n### Step 6.4: Create Blog Post\n- [ ] 1500-2500 words\n- [ ] Includes architecture diagram (embedded image)\n- [ ] Explains design decisions\n- [ ] Technical accuracy verified\n- [ ] Shared with instructor (comment access enabled)\n\n### Step 6.5: Final Checklist\n- [ ] All code files reviewed\n- [ ] No hardcoded values (except StudentId)\n- [ ] Documentation complete\n- [ ] Queries tested and working\n- [ ] Ready for grading\n\n---\n\n## Troubleshooting\n\n### CloudFormation Stack Failed\n```bash\naws cloudformation describe-stack-events \\\n  --stack-name capstone-jsanghvi \\\n  --region us-west-2 \\\n  --query 'StackEvents[?ResourceStatus==`CREATE_FAILED`]'\n```\nCheck error messages and fix template.\n\n### Glue Job Failed\n```bash\naws logs tail /aws-glue/jobs/capstone-etl-jsanghvi --since 1h\n```\nCheck for Python errors, missing dependencies, or permission issues.\n\n### No Data in Gold Tables\n```bash\naws s3 ls s3://capstone-events-jsanghvi-{ACCOUNT}/events/ --recursive\naws s3 ls s3://capstone-analytics-jsanghvi-{ACCOUNT}/bronze/ --recursive\n```\nVerify source data exists and Bronze layer was created.\n\n### Athena Query Timeout\nAdd date filter to queries, use LIMIT clause, check table partitions exist.\n\n---\n\n## Success Criteria\n\n✅ All checks completed  \n✅ All files present and validated  \n✅ All 5 SQL queries execute successfully  \n✅ Data flows through all 3 layers (Bronze → Silver → Gold)  \n✅ Query results make business sense  \n✅ Documentation complete  \n✅ Ready for grading  \n\n---\n\n**Status**: Ready for deployment  \n**Estimated Time**: 1-2 hours total  \n**Last Updated**: December 8, 2025\n